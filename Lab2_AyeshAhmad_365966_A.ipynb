{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Submission By: Ayesh Ahmad**\n",
        "\n",
        "**CMS: 365966**"
      ],
      "metadata": {
        "id": "sKZGh0ofFkl_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "obe07QEcBYoy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Linear Regression Single Prediction"
      ],
      "metadata": {
        "id": "e37DPTjA8HHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_single_prediction(w, x):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    w (ndarray): Weight vector\n",
        "    x (ndarray): Feature vector\n",
        "\n",
        "  Returns:\n",
        "    y_pred (float): Linear regression prediction for the given input.\n",
        "  \"\"\"\n",
        "  if len(w.shape) == 1 and len(x.shape) == 1:\n",
        "    y_pred = np.dot(w, x)\n",
        "    return y_pred\n",
        "  else:\n",
        "    raise ValueError(\"'w' and 'x' must be vectors.\")\n",
        "\n",
        "w = np.array([1, 2, 3])\n",
        "x = np.array([1, 2, 3])\n",
        "\n",
        "test = linear_regression_single_prediction(w, x)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6n2HG_NCF8l",
        "outputId": "c9933f3a-a6d0-4bb2-fe5a-2f24da8d8546"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Linear Regression Vector Prediction"
      ],
      "metadata": {
        "id": "lD7S_1tg8696"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_vector_prediction(w, x):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    w (ndarray): Weight vector\n",
        "    x (ndarray): Feature matrix\n",
        "\n",
        "  Returns:\n",
        "    y_pred (float): Linear regression prediction for the given input.\n",
        "  \"\"\"\n",
        "  if len(w.shape) == 1 and type(x == 'numpy.ndarray'):\n",
        "    y_pred = np.dot(w, x.T)\n",
        "    return y_pred\n",
        "  else:\n",
        "    raise ValueError(\"'w' must be a vector and 'x' must be a 'numpy.ndarray'.\")\n",
        "\n",
        "w = np.array([1, 2, 3])\n",
        "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "test = linear_regression_vector_prediction(w, x)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBicHnqVD7Cp",
        "outputId": "3766b825-876e-4bd2-fbc5-a7774c13b5a8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14 32 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Mean Squared Error"
      ],
      "metadata": {
        "id": "5FZyveTe9dE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y, y_pred):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    y (ndarray): True targets associated with the given predictions.\n",
        "    y_pred (ndarray): Predicted targets generated by the regression model.\n",
        "\n",
        "  Returns:\n",
        "    mse (float): The computed Mean Squared Error.\n",
        "  \"\"\"\n",
        "  mse = np.mean((y_pred - y) ** 2)\n",
        "  return mse\n",
        "\n",
        "y = np.array([2, 2.5, 3.5, 4.5])\n",
        "y_pred = np.array([1, 2, 3, 4])\n",
        "print(mean_squared_error(y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAYZ1lB3Tiyx",
        "outputId": "669b813d-d59f-49b6-9114-397be7552508"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Mean Squared Error Gradient"
      ],
      "metadata": {
        "id": "em2Tddrf-XxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error_gradient(x, y, y_pred):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    x (ndarray (m,n)): Feature matrix, m examples and n training features\n",
        "    y (ndarray (m,)): True targets associated with the given predictions\n",
        "    y_pred (ndarray (m,)): Predicted targets generated by the regression model, m examples\n",
        "  Returns:\n",
        "    mse_gradient (ndarray (n, )): The gradient of the cost w.r.t. the parameter w\n",
        "  \"\"\"\n",
        "  N = len(y_pred)\n",
        "  mse_gradient = (2 * np.dot((y_pred - y), x)) / N\n",
        "  return mse_gradient\n",
        "\n",
        "x = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "y_pred = np.array([4.5, 5, 6])\n",
        "y = np.array([5, 5.5, 6])\n",
        "\n",
        "print(mean_squared_error_gradient(x, y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvUn5EURNXjZ",
        "outputId": "afd9fba3-6ff2-4372-f753-11dd53df4d67"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.         -1.66666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5i. Vanilla Gradient Descent Algorithm\n"
      ],
      "metadata": {
        "id": "EnmXlNh-BJNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(w, x, y, epochs = 100, learning_rate = 0.01):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    w (ndarray): Weight vector\n",
        "    x (ndarray): Feature matrix\n",
        "    y (ndarray): True target values\n",
        "    epochs (int): Number of iterations over the entire dataset during training. Default is 100.\n",
        "    learning_rate (float): Step size for weight updates in each iteration. Default is 0.01.\n",
        "  Returns:\n",
        "    w (ndarray): Optimized weight vector\n",
        "  \"\"\"\n",
        "  print(\"+---------------+-------------------------------+\")\n",
        "  print(f\"|\\tEpoch\\t|\\t\\tLoss\\t\\t|\")\n",
        "  print(\"+---------------+-------------------------------+\")\n",
        "  for i in range(epochs):\n",
        "    y_pred = linear_regression_vector_prediction(w, x)\n",
        "    loss = mean_squared_error(y, y_pred)\n",
        "    loss_gradient = mean_squared_error_gradient(x, y, y_pred)\n",
        "    w = w - learning_rate * loss_gradient\n",
        "    print(f\"|\\t{i+1}\\t|\\t{loss}\\t|\")\n",
        "  print(\"+---------------+-------------------------------+\")\n",
        "  return w\n",
        "\n",
        "w = np.array([0, 0 , 0])\n",
        "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "y = np.array([0.85, 2, 3])\n",
        "y_pred = np.array([1, 3, 5])\n",
        "\n",
        "updated_w = gradient_descent(w, x, y, learning_rate=0.01, epochs=10)\n",
        "\n",
        "print(\"Updated Weights: \", updated_w)"
      ],
      "metadata": {
        "id": "PIWFv2Znf9tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248f7c6c-bc1d-4d08-aa51-1d164606fc46"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t4.574166666666667\t|\n",
            "|\t2\t|\t3.5254466666666673\t|\n",
            "|\t3\t|\t1.3806259306666675\t|\n",
            "|\t4\t|\t1.376739359505067\t|\n",
            "|\t5\t|\t0.539851000357772\t|\n",
            "|\t6\t|\t0.6931694666200549\t|\n",
            "|\t7\t|\t0.33459876093368407\t|\n",
            "|\t8\t|\t0.4581345981024986\t|\n",
            "|\t9\t|\t0.29117782528901537\t|\n",
            "|\t10\t|\t0.3668312458997769\t|\n",
            "+---------------+-------------------------------+\n",
            "Updated Weights:  [0.17165322 0.14815694 0.12466066]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5ii. Stochastic Gradient Descent Algorithm"
      ],
      "metadata": {
        "id": "38vu4lNnK49_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_mean_squared_error_gradient(x, y, y_pred):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    x (ndarray (n,)): Feature vector for a single example\n",
        "    y (float): True target for the example\n",
        "    y_pred (float): Predicted target for the example\n",
        "  Returns:\n",
        "    mse_gradient (ndarray (n, )): The gradient of the cost w.r.t. the parameter w for a single example\n",
        "  \"\"\"\n",
        "  mse_gradient = 2 * (y_pred - y) * x\n",
        "  return mse_gradient\n",
        "\n",
        "x = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "y_pred = np.array([4.5, 5, 6])\n",
        "y = np.array([5, 5.5, 6])\n",
        "N = len(y)\n",
        "\n",
        "for i in range(N):\n",
        "  gradient = stochastic_mean_squared_error_gradient(x[i], y[i], y_pred[i])\n",
        "  print(f\"Training Example {i+1}: {gradient}\")\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def stochastic_gradient_descent(w, x, y, epochs=100, learning_rate=0.01):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    w (ndarray): Weight vector\n",
        "    x (ndarray): Feature matrix\n",
        "    y (ndarray): True target values\n",
        "    epochs (int): Number of passes over the entire dataset during training. Default is 100.\n",
        "    learning_rate (float): Step size for weight updates in each iteration. Default is 0.01.\n",
        "  Returns:\n",
        "    w (ndarray): Optimized weight vector\n",
        "  \"\"\"\n",
        "  N = len(y)\n",
        "  print(\"\\n+---------------+-------------------------------+\")\n",
        "  print(f\"|\\tEpoch\\t|\\t\\tLoss\\t\\t|\")\n",
        "  print(\"+---------------+-------------------------------+\")\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(N):\n",
        "      y_pred = np.dot(w, x[i])\n",
        "      loss = (y_pred - y[i])**2\n",
        "      total_loss += loss\n",
        "      gradient = stochastic_mean_squared_error_gradient(x[i], y[i], y_pred)\n",
        "      w = w - learning_rate * gradient\n",
        "    average_loss = total_loss / N\n",
        "    print(f\"|\\t{epoch+1}\\t|\\t{average_loss}\\t|\")\n",
        "  print(\"+---------------+-------------------------------+\")\n",
        "  return w\n",
        "\n",
        "w = np.array([0, 0, 0])\n",
        "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "y = np.array([0.85, 2, 3])\n",
        "\n",
        "updated_w = stochastic_gradient_descent(w, x, y, learning_rate=0.001, epochs=10)\n",
        "\n",
        "print(\"Updated Weights:\", updated_w)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC7X5jmqK8XQ",
        "outputId": "6e58374e-f30a-48af-a91a-a049c653f368"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Example 1: [-1. -2.]\n",
            "Training Example 2: [-2. -3.]\n",
            "Training Example 3: [0. 0.]\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t3.4875982009523203\t|\n",
            "|\t2\t|\t0.8962700205458672\t|\n",
            "|\t3\t|\t0.2342247756430644\t|\n",
            "|\t4\t|\t0.06443068464300385\t|\n",
            "|\t5\t|\t0.02055274294055513\t|\n",
            "|\t6\t|\t0.009044324548857117\t|\n",
            "|\t7\t|\t0.0059377085445734635\t|\n",
            "|\t8\t|\t0.0050520169782082005\t|\n",
            "|\t9\t|\t0.004773255245258796\t|\n",
            "|\t10\t|\t0.004670000067606186\t|\n",
            "+---------------+-------------------------------+\n",
            "Updated Weights: [0.10240394 0.12466721 0.14693049]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Application of Linear Regression"
      ],
      "metadata": {
        "id": "f1g5rXF3Qg4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##i. Loading dataset and normalizing"
      ],
      "metadata": {
        "id": "ZHtHE2T9QoDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "california_housing = datasets.fetch_california_housing()\n",
        "X = california_housing.data  # Feature matrix\n",
        "y = california_housing.target  # Target values (median house values)\n",
        "\n",
        "def normalize_data(X):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    X (ndarray): Feature matrix\n",
        "  Returns:\n",
        "    X_scaled_with_bias (ndarray): Normalized feature matrix with a bias column added\n",
        "    scaler (StandardScaler): StandardScaler object fitted to the input data\n",
        "  \"\"\"\n",
        "  scaler = StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "  # Adding a bias column\n",
        "  ones_column = np.ones((X_scaled.shape[0], 1))\n",
        "  X_scaled_with_bias = np.hstack((ones_column, X_scaled))\n",
        "\n",
        "  return X_scaled_with_bias, scaler"
      ],
      "metadata": {
        "id": "aK64xyqAQj9j"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ii. Creating a Linear Regression Model"
      ],
      "metadata": {
        "id": "qG--9YAgRXR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression(X, y, epochs = 20, learning_rate = 0.1, gd_type = \"vanilla\"):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    X (ndarray): Feature matrix\n",
        "    y (ndarray): True target values\n",
        "    epochs (int): Number of passes over the entire dataset during training. Default is 10.\n",
        "    type (str): Type of gradient descent to use. 'vanilla'/'stochastic'. Default is 'vanilla'.\n",
        "  Returns:\n",
        "    w (ndarray): Optimized weight vector\n",
        "  \"\"\"\n",
        "  X_normalized, scaler = normalize_data(X)\n",
        "  w = np.zeros(X_normalized.shape[1])\n",
        "\n",
        "  if gd_type == 'vanilla':\n",
        "    w = gradient_descent(w, X_normalized, y, epochs, learning_rate)\n",
        "  elif gd_type == 'stochastic':\n",
        "    w = stochastic_gradient_descent(w, X_normalized, y, epochs, learning_rate)\n",
        "  else:\n",
        "    raise ValueError(\"Invalid gd_type. Choose 'vanilla' or 'stochastic'.\")\n",
        "\n",
        "  return w"
      ],
      "metadata": {
        "id": "XK3az89vRWM1"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iii. Linear Regression using Vanilla Gradient Descent"
      ],
      "metadata": {
        "id": "D6AdiEJRUjAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "linear_regression_vanilla = linear_regression(X, y, epochs = 1, learning_rate = 0.1, gd_type='vanilla')\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(\"Optimized weights:\", linear_regression_vanilla)\n",
        "print(\"Execution time:\", execution_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crKMWZr2Usql",
        "outputId": "12d84997-ac69-4cee-e706-6e967eefb506"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t5.610483198987253\t|\n",
            "+---------------+-------------------------------+\n",
            "Optimized weights: [ 0.41371163  0.15879788  0.02437637  0.03506748 -0.01077781 -0.00568879\n",
            " -0.00547825 -0.03327012 -0.01060843]\n",
            "Execution time: 0.013284921646118164 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iv. Linear Regression using Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "339M4ch9V0LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "linear_regression_stochastic = linear_regression(X, y, epochs = 1, learning_rate = 0.0001, gd_type='stochastic')\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(\"Optimized weights:\", linear_regression_stochastic)\n",
        "print(\"Execution time:\", execution_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "280fxRs9V6De",
        "outputId": "76af439e-fdf5-47a5-dc2b-5685522e2e41"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t1.1828672141252614\t|\n",
            "+---------------+-------------------------------+\n",
            "Optimized weights: [ 1.92453705e+00  7.85976652e-01  2.38430376e-01 -3.18194993e-04\n",
            " -1.75919313e-02  2.60736288e-02  4.08018064e-03 -2.06013965e-01\n",
            " -2.38376653e-01]\n",
            "Execution time: 0.12233567237854004 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Results\n",
        "| Gradient Descent Type | Step Size | Epochs | Loss after Training | Execution Time (s) |\n",
        "|-------------|-----------|---------|------------|----------|\n",
        "| Vanilla     | 0.1       | 20      | 0.599      |  0.052   |\n",
        "| Vanilla     | 0.1       | 1       | 5.610      |  0.013   |\n",
        "| Stochastic  | 0.0001    | 20      | 0.515      |  2.844   |\n",
        "| Stochastic  | 0.0001    | 1       | 1.182      |  0.122   |\n",
        "\n",
        "\n",
        "##- Vanilla Gradient Descent\n",
        "Vanilla GD generally requires more epochs to do well, performing very poorly with just 1 epoch. However, Vanilla GD is **extremely fast**, even with a high number of epochs, due to the relatively smaller size of the dataset allowing for a quicker batch process.\n",
        "\n",
        "##- Stochastic Gradient Descent\n",
        "Stochastic GD performs well overall, with either 1, or 20 epochs. It required a thousandth of the learning rate of vanilla GD as well. The only downside is that it is noticably slower.\n",
        "\n",
        "##Conclusion\n",
        "Given the dataset we were tasked to work on, Stochastic Gradient Descent is a good choice for us. Its loss is significantly better than vanilla GD at lower epochs, and comparable yet still better at higher epochs. The only downside to it is the execution time which is not of much concern in this application as 2 seconds is a bearable amount of time.\n",
        "\n",
        "If the dataset were to be significantly larger, or if we were to increase the number of epochs significantly, then the execution time would prove too unbearably large to dismiss, and Vanilla GD would be a better option."
      ],
      "metadata": {
        "id": "cNbSP7SUYOSz"
      }
    }
  ]
}