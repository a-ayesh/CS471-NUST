{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOeomidW8WeR"
      },
      "source": [
        "## CS-471: Machine Learning\n",
        "### **Submitted By**:\n",
        "#### **Name**: Ayesh Ahmad\n",
        "#### **CMS**: 365966\n",
        "#### **Class**: BESE-12A\n",
        "---\n",
        "## Lab 11\n",
        "#### Learn back propagation from through coding exercises at different levels of complexity to help grasp the concept of backpropagation:\n",
        "- #### Basic Back Propagation: This exercise introduces the concept of backpropagation and its fundamental steps.\n",
        "- #### Back Propagation for Multi-layer Neural Network: In this exercise, you'll deepen your understanding of backpropagation by implementing it for a multi-layer neural network.\n",
        "\n",
        "--- \n",
        "\n",
        "## Exercise 1: Basic Back Propagation\n",
        "\n",
        "##### Step 1: Importing Libraries\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tUkVzyIv9ZLw"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLuUQwJf8jL_"
      },
      "source": [
        "##### Step 2: Defining the Network Architecture\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = 1\n",
        "hidden_size = 1\n",
        "output_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 3: Initializing Weights and Biases\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)  \n",
        "\n",
        "# Hidden layer parameters\n",
        "W1 = np.random.rand(input_size, hidden_size)\n",
        "b1 = np.random.rand(1, hidden_size)\n",
        "\n",
        "# Output layer parameters\n",
        "W2 = np.random.rand(hidden_size, output_size)\n",
        "b2 = np.random.rand(1, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 4: Defining the Activation Function (Sigmoid)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 5: Forward Pass\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward(input_data):\n",
        "    global W1, W2, b1, b2, a1\n",
        "\n",
        "    # Hidden layer\n",
        "    z1 = np.dot(input_data, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    \n",
        "    # Output layer\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    output = sigmoid(z2)\n",
        "    \n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 6: Loss Function (Mean Squared Error)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(output, target):\n",
        "    return np.mean(np.square(output - target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 7: Backward Pass (Backpropagation)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward(input_data, target, output, learning_rate):\n",
        "    global W1, W2, b1, b2\n",
        "    \n",
        "    # Backpropagate through output layer\n",
        "    output_error = output - target\n",
        "    dz2 = output_error * output * (1 - output)\n",
        "    dW2 = np.dot(a1.T, dz2)\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "    # Backpropagate through hidden layer\n",
        "    hidden_error = np.dot(dz2, W2.T)\n",
        "    dz1 = hidden_error * a1 * (1 - a1)  \n",
        "    dW1 = np.dot(input_data.T, dz1)\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 8: Training Loop\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100: Loss = 0.24790425470680683\n",
            "Epoch 200: Loss = 0.24419961091272355\n",
            "Epoch 300: Loss = 0.2412532405672978\n",
            "Epoch 400: Loss = 0.23724138675368395\n",
            "Epoch 500: Loss = 0.2316736164398244\n",
            "Epoch 600: Loss = 0.2239170347183977\n",
            "Epoch 700: Loss = 0.21315264327149527\n",
            "Epoch 800: Loss = 0.19851427288626994\n",
            "Epoch 900: Loss = 0.17955877188368535\n",
            "Epoch 1000: Loss = 0.15699501239343513\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1000\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Input and target data\n",
        "input_data = np.array([[0], [1]])\n",
        "target = np.array([[0], [1]])\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    output = forward(input_data)\n",
        "    current_loss = loss(output, target)\n",
        "    backward(input_data, target, output, learning_rate)\n",
        "\n",
        "    # Print current loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {current_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 9: Testing the Trained Model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction for input 0.5: 0.5366546209733737\n"
          ]
        }
      ],
      "source": [
        "test_data = np.array([[0.5]])\n",
        "print(f\"Prediction for input {test_data[0][0]}: {forward(test_data)[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "## Exercise 2: Back Propagation for Multi-layer Neural Network\n",
        "\n",
        "##### Step 1: Importing Libraries\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 2: Defining the Network Architecture\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = 4  \n",
        "hidden_size1 = 10 \n",
        "hidden_size2 = 10\n",
        "output_size = 3 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 3: Initializing Weights and Biases\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)  \n",
        "\n",
        "# Hidden layer 1 parameters\n",
        "W1 = np.random.randn(input_size, hidden_size1)\n",
        "b1 = np.random.randn(1, hidden_size1)\n",
        "\n",
        "# Hidden layer 2 parameters\n",
        "W2 = np.random.randn(hidden_size1, hidden_size2)\n",
        "b2 = np.random.randn(1, hidden_size2)\n",
        "\n",
        "# Output layer parameters\n",
        "W3 = np.random.randn(hidden_size2, output_size)\n",
        "b3 = np.random.randn(1, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 4: Defining the Activation Function\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 5: Forward Pass\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward(input_data):\n",
        "    global W1, W2, W3, b1, b2, b3, a1, z1, a2, z2\n",
        "\n",
        "    # Hidden layer 1\n",
        "    z1 = np.dot(input_data, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "\n",
        "    # Hidden layer 2\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = relu(z2)\n",
        "\n",
        "    # Output layer\n",
        "    z3 = np.dot(a2, W3) + b3\n",
        "    output = softmax(z3)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 6: Loss Function (Cross Entropy Loss)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(output, target):\n",
        "    m = target.shape[0]\n",
        "    log_likelihood = -np.log(output[np.arange(m), target.argmax(axis=1)])\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 7: Backward Pass (Backpropagation)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward(input_data, target, output, learning_rate):\n",
        "    global W1, W2, W3, b1, b2, b3\n",
        "\n",
        "    m = input_data.shape[0]\n",
        "\n",
        "    # Backpropagate through output layer\n",
        "    dz3 = output - target\n",
        "    dW3 = (1 / m) * np.dot(a2.T, dz3)\n",
        "    db3 = (1 / m) * np.sum(dz3, axis=0, keepdims=True)\n",
        "    \n",
        "    # Backpropagate through hidden layer 2\n",
        "    dA2 = np.dot(dz3, W3.T)\n",
        "    dz2 = dA2 * (z2 > 0)\n",
        "    dW2 = (1 / m) * np.dot(a1.T, dz2)\n",
        "    db2 = (1 / m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "    \n",
        "    # Backpropagate through hidden layer 1\n",
        "    dA1 = np.dot(dz2, W2.T)\n",
        "    dz1 = dA1 * (z1 > 0)\n",
        "    dW1 = (1 / m) * np.dot(input_data.T, dz1)\n",
        "    db1 = (1 / m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "    \n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 8: Training Loop\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, Loss: 0.11834210622229858\n",
            "Epoch 200, Loss: 0.11205369203497231\n",
            "Epoch 300, Loss: 0.10700120093391824\n",
            "Epoch 400, Loss: 0.10275262896894041\n",
            "Epoch 500, Loss: 0.09908939510165261\n",
            "Epoch 600, Loss: 0.09587098897416459\n",
            "Epoch 700, Loss: 0.09300394524293992\n",
            "Epoch 800, Loss: 0.09042092363773625\n",
            "Epoch 900, Loss: 0.08807544602450816\n",
            "Epoch 1000, Loss: 0.08593401950838968\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y_encoded = np.eye(output_size)[y]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=0)\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    y_pred = forward(X_train)\n",
        "    train_loss = loss(y_pred, y_train)\n",
        "    backward(X_train, y_train, y_pred, learning_rate)\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {train_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 9: Testing the Trained Model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred_test = forward(X_test)\n",
        "y_pred_labels = np.argmax(y_pred_test, axis=1)\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "## Results & Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Exercise 1: Basic Back Propagation\n",
        "- **Architecture**: Single-layer neural network with 1 input neuron, 1 hidden neuron, and 1 output neuron.\n",
        "- **Training**: The network was trained for 1000 epochs using backpropagation and achieved a final loss of 0.156.\n",
        "- **Prediction**: For input 0.5, the network predicted an output of approximately 0.537.\n",
        "\n",
        "##### Exercise 2: Back Propagation for Multi-layer Neural Network\n",
        "- **Architecture**: Multi-layer neural network with 4 input neurons, 10 neurons in each of the two hidden layers, and 3 output neurons.\n",
        "- **Training**: The network was trained for 1000 epochs using backpropagation and achieved a final loss of 0.086.\n",
        "- **Performance**: The model achieved perfect precision, recall, and F1-score on the test set, indicating that it successfully learned to classify the Iris dataset.\n",
        "\n",
        "##### Conclusion\n",
        "- Both the basic and multi-layer neural networks were able to learn and perform well on their datasets, achieving high accuracy and demonstrating the effectiveness of backpropagation in training neural networks.\n",
        "- The multi-layer neural network with two hidden layers showed superior performance, highlighting the benefits of deeper networks in learning complex patterns in data.\n",
        "- Further tuning of hyperparameters such as learning rate, number of epochs, and network architecture could potentially improve the performance even more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
