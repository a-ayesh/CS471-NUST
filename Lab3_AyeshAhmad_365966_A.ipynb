{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Submission By: Ayesh Ahmad**\n",
        "\n",
        "**CMS: 365966**"
      ],
      "metadata": {
        "id": "sKZGh0ofFkl_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "obe07QEcBYoy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Logistic Regression Single Prediction"
      ],
      "metadata": {
        "id": "e37DPTjA8HHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_single_prediction(w, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      w (ndarray): Weight vector\n",
        "      x (ndarray): Feature vector\n",
        "\n",
        "    Returns:\n",
        "      y_pred (int): Binary prediction (0 or 1) for the given input.\n",
        "    \"\"\"\n",
        "    if len(w.shape) == 1 and len(x.shape) == 1:\n",
        "        z = np.dot(w, x)\n",
        "        y_pred_prob = 1 / (1 + np.exp(-z))\n",
        "        y_pred = 1 if y_pred_prob >= 0.5 else 0\n",
        "        return y_pred\n",
        "    else:\n",
        "        raise ValueError(\"'w' and 'x' must be vectors.\")\n",
        "\n",
        "w = np.array([1, 2, 3])\n",
        "x = np.array([1, 2, 3])\n",
        "\n",
        "test = logistic_regression_single_prediction(w, x)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6n2HG_NCF8l",
        "outputId": "a3e4a84f-a8d9-4aa4-a387-fef6b3fe9778"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Logistic Regression Vector Prediction"
      ],
      "metadata": {
        "id": "lD7S_1tg8696"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_vector_prediction(w, X):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      w (ndarray): Weight vector (shape: (n_features,))\n",
        "      X (ndarray): Feature matrix (shape: (n_samples, n_features))\n",
        "\n",
        "    Returns:\n",
        "      y_pred_prob (ndarray): Probabalistic prediction vector for the given input (shape: (n_samples,)).\n",
        "    \"\"\"\n",
        "    if len(w.shape) == 1 and len(X.shape) == 2 and w.shape[0] == X.shape[1]:\n",
        "        z = np.dot(X, w)\n",
        "        y_pred_prob = 1 / (1 + np.exp(-z))\n",
        "        return y_pred_prob\n",
        "    else:\n",
        "        raise ValueError(\"'w' must be a weight vector (shape: (n_features,)) and 'X' must be a feature matrix (shape: (n_samples, n_features)).\")\n",
        "\n",
        "w = np.array([0.5, 0.3, -0.2])\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "predictions = logistic_regression_vector_prediction(w, X)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBicHnqVD7Cp",
        "outputId": "69510274-1e7e-43ad-f332-adcd15b2a5ef"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.62245933 0.90887704 0.9836975 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Logistic Loss"
      ],
      "metadata": {
        "id": "5FZyveTe9dE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_loss(w, X, y):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      w (ndarray): Weight vector (shape: (n_features,))\n",
        "      X (ndarray): Feature matrix (shape: (n_samples, n_features))\n",
        "      y (ndarray): Label vector (shape: (n_samples,))\n",
        "\n",
        "    Returns:\n",
        "      loss (float): Logistic Loss for the given input.\n",
        "    \"\"\"\n",
        "    y_pred_prob = logistic_regression_vector_prediction(w, X)\n",
        "    loss = -np.mean(y * np.log(y_pred_prob) + (1 - y) * np.log(1 - y_pred_prob))\n",
        "    return loss\n",
        "\n",
        "w = np.array([0.5, 0.3, -0.2])\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "y = np.array([1, 0, 1])\n",
        "loss = logistic_loss(w, X, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAYZ1lB3Tiyx",
        "outputId": "5fc33032-d8c9-42b6-d3da-b4d28b11550b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9620197653436594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Gradient of Logistic Loss"
      ],
      "metadata": {
        "id": "em2Tddrf-XxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_loss_gradient(w, X, y):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      w (ndarray): Weight vector (shape: (n_features,))\n",
        "      X (ndarray): Feature matrix (shape: (n_samples, n_features))\n",
        "      y (ndarray): Label vector (shape: (n_samples,))\n",
        "\n",
        "    Returns:\n",
        "      gradient (ndarray): Gradient of Logistic Loss with respect to the weight vector (shape: (n_features,)).\n",
        "    \"\"\"\n",
        "    y_pred_prob = logistic_regression_vector_prediction(w, X)\n",
        "    gradient = np.dot(X.T, y_pred_prob - y) / len(y)\n",
        "    return gradient\n",
        "\n",
        "w = np.array([0.5, 0.3, -0.2])\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "y = np.array([1, 0, 1])\n",
        "gradient = logistic_loss_gradient(w, X, y)\n",
        "print(gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvUn5EURNXjZ",
        "outputId": "b99713f0-f10b-4432-dba5-ffb9424e9c03"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.04795    1.21962795 1.39130591]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Gradient Descent Algorithm\n"
      ],
      "metadata": {
        "id": "EnmXlNh-BJNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_logistic_regression(w, X, y, learning_rate, stopping_criterion, batch_size=None, max_iters=20):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      w (ndarray): Initial weight vector (shape: (n_features,))\n",
        "      X (ndarray): Feature matrix (shape: (n_samples, n_features))\n",
        "      y (ndarray): Label vector (shape: (n_samples,))\n",
        "      learning_rate (float): Learning rate for gradient descent\n",
        "      stopping_criterion (float): Value indicating the threshold for stopping the algorithm\n",
        "      batch_size (int): Size of the mini-batches for stochastic gradient descent. Default is None (vanilla gradient descent).\n",
        "      max_iters (int): Maximum number of iterations. Default is 1000.\n",
        "\n",
        "    Returns:\n",
        "      w_optimized (ndarray): Optimized weight vector.\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = len(y)\n",
        "\n",
        "    w_optimized = w.copy()\n",
        "    prev_loss = np.inf\n",
        "\n",
        "    print(\"\\n+---------------+-------------------------------+\")\n",
        "    print(f\"|\\tEpoch\\t|\\t\\tLoss\\t\\t|\")\n",
        "    print(\"+---------------+-------------------------------+\")\n",
        "\n",
        "    for epoch in range(1, max_iters + 1):\n",
        "        indices = np.random.choice(len(y), batch_size, replace=False)\n",
        "        X_batch = X[indices]\n",
        "        y_batch = y[indices]\n",
        "\n",
        "        gradient = logistic_loss_gradient(w_optimized, X_batch, y_batch)\n",
        "        w_optimized -= learning_rate * gradient\n",
        "\n",
        "        current_loss = logistic_loss(w_optimized, X, y)\n",
        "        print(f\"|\\t{epoch}\\t|\\t{current_loss}\\t|\")\n",
        "\n",
        "        if abs(prev_loss - current_loss) < stopping_criterion:\n",
        "            break\n",
        "        prev_loss = current_loss\n",
        "\n",
        "    print(\"+---------------+-------------------------------+\")\n",
        "    return w_optimized\n",
        "\n",
        "# Vanilla Gradient Descent\n",
        "w_initial = np.array([0.5, 0.3, -0.2])\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "y = np.array([1, 0, 1])\n",
        "learning_rate = 0.01\n",
        "stopping_criterion = 1e-6\n",
        "w_optimized = gradient_descent_logistic_regression(w_initial, X, y, learning_rate, stopping_criterion)\n",
        "print(\"Vanilla Gradient Descent - Final Weights:\", w_optimized, \"\\n\\n\")\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "batch_size = 1\n",
        "w_initial = np.array([0.5, 0.3, -0.2])\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "y = np.array([1, 0, 1])\n",
        "learning_rate = 0.01\n",
        "stopping_criterion = 1e-6\n",
        "w_optimized = gradient_descent_logistic_regression(w_initial, X, y, learning_rate, stopping_criterion, batch_size)\n",
        "print(\"Stochastic Gradient Descent - Final Weights:\", w_optimized)"
      ],
      "metadata": {
        "id": "PIWFv2Znf9tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f25ca0b-e29f-4332-d88f-b8a0dc42c420"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.9177997620048218\t|\n",
            "|\t2\t|\t0.8776322597924787\t|\n",
            "|\t3\t|\t0.8417320992010113\t|\n",
            "|\t4\t|\t0.8102285185170045\t|\n",
            "|\t5\t|\t0.7831385499494887\t|\n",
            "|\t6\t|\t0.7603493923032522\t|\n",
            "|\t7\t|\t0.7416155999228593\t|\n",
            "|\t8\t|\t0.7265740855060026\t|\n",
            "|\t9\t|\t0.7147752814186551\t|\n",
            "|\t10\t|\t0.7057241847611401\t|\n",
            "|\t11\t|\t0.6989225630912159\t|\n",
            "|\t12\t|\t0.6939043447145256\t|\n",
            "|\t13\t|\t0.6902594378088655\t|\n",
            "|\t14\t|\t0.6876451162496515\t|\n",
            "|\t15\t|\t0.6857870560354592\t|\n",
            "|\t16\t|\t0.6844734413465866\t|\n",
            "|\t17\t|\t0.6835455177141858\t|\n",
            "|\t18\t|\t0.6828871770397896\t|\n",
            "|\t19\t|\t0.6824151855124928\t|\n",
            "|\t20\t|\t0.6820708422140909\t|\n",
            "+---------------+-------------------------------+\n",
            "Vanilla Gradient Descent - Final Weights: [ 0.39098838  0.18247049 -0.3260474 ] \n",
            "\n",
            "\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.8090238895803048\t|\n",
            "|\t2\t|\t0.8203225592970829\t|\n",
            "|\t3\t|\t0.8309444981213248\t|\n",
            "|\t4\t|\t0.8594105469662194\t|\n",
            "|\t5\t|\t0.8679770279421054\t|\n",
            "|\t6\t|\t0.7448450833300314\t|\n",
            "|\t7\t|\t0.6868999658166662\t|\n",
            "|\t8\t|\t0.6968188269548653\t|\n",
            "|\t9\t|\t0.6869558254242615\t|\n",
            "|\t10\t|\t0.6867441718627211\t|\n",
            "|\t11\t|\t0.6966875196387563\t|\n",
            "|\t12\t|\t0.7142238960675539\t|\n",
            "|\t13\t|\t0.6828018854356781\t|\n",
            "|\t14\t|\t0.6916328846459928\t|\n",
            "|\t15\t|\t0.6905728222851747\t|\n",
            "|\t16\t|\t0.6854761643542973\t|\n",
            "|\t17\t|\t0.6996726715599347\t|\n",
            "|\t18\t|\t0.6861096071590715\t|\n",
            "|\t19\t|\t0.6823033533764419\t|\n",
            "|\t20\t|\t0.7095488986023165\t|\n",
            "+---------------+-------------------------------+\n",
            "Stochastic Gradient Descent - Final Weights: [ 0.36998351  0.14938377 -0.37121597]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Application of Logistic Regression"
      ],
      "metadata": {
        "id": "f1g5rXF3Qg4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##i. Loading dataset and normalizing"
      ],
      "metadata": {
        "id": "ZHtHE2T9QoDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
        "\n",
        "# Pandas data frame with feature vectors\n",
        "X = mnist.data\n",
        "\n",
        "# Labels\n",
        "y = mnist.target\n",
        "\n",
        "# Labels converted to integers\n",
        "y = y.astype(int)\n",
        "\n",
        "# Split the dataset into training and test sets (using 50,000 examples for training)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50000, random_state=42)\n",
        "\n",
        "# Normalize the feature vectors\n",
        "scaler = StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "print(\"Training set shape:\", X_train_normalized.shape, y_train.shape)\n",
        "print(\"Test set shape:\", X_test_normalized.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "aK64xyqAQj9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68edff3a-7106-41ba-ce7c-ad9a67c723eb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (50000, 784) (50000,)\n",
            "Test set shape: (20000, 784) (20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ii. Creating a Linear Regression Model"
      ],
      "metadata": {
        "id": "qG--9YAgRXR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "def logistic_regression_one_vs_rest(X_train, y_train, X_test, y_test, learning_rate, stopping_criterion, batch_size=None, max_iters=20):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X_train (ndarray): Training feature matrix (shape: (n_train_samples, n_features))\n",
        "      y_train (ndarray): Training label vector (shape: (n_train_samples,))\n",
        "      X_test (ndarray): Test feature matrix (shape: (n_test_samples, n_features))\n",
        "      y_test (ndarray): Test label vector (shape: (n_test_samples,))\n",
        "      learning_rate (float): Learning rate for gradient descent\n",
        "      stopping_criterion (float): Value indicating the threshold for stopping the algorithm\n",
        "      batch_size (int): Size of the mini-batches for stochastic gradient descent. Default is None (vanilla gradient descent).\n",
        "      max_iters (int): Maximum number of iterations. Default is 20.\n",
        "\n",
        "    Returns:\n",
        "      accuracies (list): List of accuracies for each class\n",
        "      precisions (list): List of precisions for each class\n",
        "      recalls (list): List of recalls for each class\n",
        "      f1_scores (list): List of F-1 scores for each class\n",
        "    \"\"\"\n",
        "    # Binarize labels\n",
        "    lb = LabelBinarizer()\n",
        "    y_train_bin = lb.fit_transform(y_train)\n",
        "\n",
        "    # Initialize lists for evaluation metrics\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    # Train one-vs-rest logistic regression for each class\n",
        "    for i in range(10):  # 10 classes in MNIST\n",
        "        print(f\"\\n\\t\\tTraining for class {i}\")\n",
        "        y_train_class = y_train_bin[:, i]\n",
        "        w_initial = np.zeros(X_train.shape[1])\n",
        "        w_optimized = gradient_descent_logistic_regression(w_initial, X_train, y_train_class, learning_rate, stopping_criterion, batch_size, max_iters)\n",
        "\n",
        "        # Predictions for current class\n",
        "        y_pred_prob = logistic_regression_vector_prediction(w_optimized, X_test)\n",
        "        y_pred_class = (y_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "        # Evaluation metrics for current class\n",
        "        accuracies.append(accuracy_score(y_test == i, y_pred_class))\n",
        "        precisions.append(precision_score(y_test == i, y_pred_class))\n",
        "        recalls.append(recall_score(y_test == i, y_pred_class))\n",
        "        f1_scores.append(f1_score(y_test == i, y_pred_class))\n",
        "\n",
        "    return accuracies, precisions, recalls, f1_scores"
      ],
      "metadata": {
        "id": "XK3az89vRWM1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iii. Logistic Regression using Vanilla Gradient Descent"
      ],
      "metadata": {
        "id": "D6AdiEJRUjAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "stopping_criterion = 1e-6\n",
        "batch_size = None\n",
        "max_iters = 20\n",
        "\n",
        "start_time = time.time()\n",
        "accuracies, precisions, recalls, f1_scores = logistic_regression_one_vs_rest(X_train_normalized, y_train, X_test_normalized, y_test, learning_rate, stopping_criterion, batch_size, max_iters)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Display results\n",
        "print(\"\\nExecution Time: \", execution_time, \"\\n\")\n",
        "print(\"+-------+------------+-----------+--------+-----------+\")\n",
        "print(\"| Class |  Accuracy  | Precision | Recall |  F1-Score |\")\n",
        "print(\"+-------+------------+-----------+--------+-----------+\")\n",
        "for i in range(10):\n",
        "    print(f\"|   {i}   |  {accuracies[i]:.4f}    |  {precisions[i]:.4f}   | {recalls[i]:.4f} |  {f1_scores[i]:.4f}   |\")\n",
        "print(\"+-------+------------+-----------+--------+-----------+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crKMWZr2Usql",
        "outputId": "2de28940-7f29-4a6e-d1c2-07c8a0b4c9cd"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t\tTraining for class 0\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6771124251481307\t|\n",
            "|\t2\t|\t0.6637359671670245\t|\n",
            "|\t3\t|\t0.6524949581048886\t|\n",
            "|\t4\t|\t0.6429595878897816\t|\n",
            "|\t5\t|\t0.6347873860706502\t|\n",
            "|\t6\t|\t0.6277098994911612\t|\n",
            "|\t7\t|\t0.6215180113150275\t|\n",
            "|\t8\t|\t0.6160487947420292\t|\n",
            "|\t9\t|\t0.6111747831715824\t|\n",
            "|\t10\t|\t0.6067956081431392\t|\n",
            "|\t11\t|\t0.6028316296850545\t|\n",
            "|\t12\t|\t0.5992191320981775\t|\n",
            "|\t13\t|\t0.5959067090818533\t|\n",
            "|\t14\t|\t0.592852538202656\t|\n",
            "|\t15\t|\t0.5900223163273051\t|\n",
            "|\t16\t|\t0.587387686087994\t|\n",
            "|\t17\t|\t0.5849250282269338\t|\n",
            "|\t18\t|\t0.582614527930618\t|\n",
            "|\t19\t|\t0.5804394476093262\t|\n",
            "|\t20\t|\t0.5783855562877406\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 1\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6805177569600246\t|\n",
            "|\t2\t|\t0.6696428145028277\t|\n",
            "|\t3\t|\t0.6602276771781133\t|\n",
            "|\t4\t|\t0.652027571100754\t|\n",
            "|\t5\t|\t0.6448408578447837\t|\n",
            "|\t6\t|\t0.6385021672087478\t|\n",
            "|\t7\t|\t0.6328760903140193\t|\n",
            "|\t8\t|\t0.6278517023159236\t|\n",
            "|\t9\t|\t0.6233379629259989\t|\n",
            "|\t10\t|\t0.6192599333669272\t|\n",
            "|\t11\t|\t0.6155557041314939\t|\n",
            "|\t12\t|\t0.612173918524831\t|\n",
            "|\t13\t|\t0.609071784259654\t|\n",
            "|\t14\t|\t0.6062134792943823\t|\n",
            "|\t15\t|\t0.6035688734879507\t|\n",
            "|\t16\t|\t0.6011125020822263\t|\n",
            "|\t17\t|\t0.5988227395723567\t|\n",
            "|\t18\t|\t0.596681132982645\t|\n",
            "|\t19\t|\t0.5946718620634249\t|\n",
            "|\t20\t|\t0.5927813007246683\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 2\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6849329204888235\t|\n",
            "|\t2\t|\t0.6775736833962641\t|\n",
            "|\t3\t|\t0.6709606550253335\t|\n",
            "|\t4\t|\t0.6649978293501113\t|\n",
            "|\t5\t|\t0.6596014115941178\t|\n",
            "|\t6\t|\t0.6546988367907348\t|\n",
            "|\t7\t|\t0.6502275944888507\t|\n",
            "|\t8\t|\t0.6461340024760064\t|\n",
            "|\t9\t|\t0.6423720231354942\t|\n",
            "|\t10\t|\t0.6389021755353169\t|\n",
            "|\t11\t|\t0.6356905673224373\t|\n",
            "|\t12\t|\t0.6327080518088759\t|\n",
            "|\t13\t|\t0.6299295047910999\t|\n",
            "|\t14\t|\t0.62733321014807\t|\n",
            "|\t15\t|\t0.6249003411301575\t|\n",
            "|\t16\t|\t0.622614524074703\t|\n",
            "|\t17\t|\t0.6204614721431091\t|\n",
            "|\t18\t|\t0.618428678001602\t|\n",
            "|\t19\t|\t0.6165051558335659\t|\n",
            "|\t20\t|\t0.6146812244972476\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 3\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6857733954121654\t|\n",
            "|\t2\t|\t0.679115733924755\t|\n",
            "|\t3\t|\t0.6730949068249682\t|\n",
            "|\t4\t|\t0.6676395199462926\t|\n",
            "|\t5\t|\t0.6626857898885989\t|\n",
            "|\t6\t|\t0.6581770766313244\t|\n",
            "|\t7\t|\t0.6540633100558308\t|\n",
            "|\t8\t|\t0.6503003696815245\t|\n",
            "|\t9\t|\t0.6468494600106396\t|\n",
            "|\t10\t|\t0.6436765095284969\t|\n",
            "|\t11\t|\t0.6407516102142686\t|\n",
            "|\t12\t|\t0.6380485062477677\t|\n",
            "|\t13\t|\t0.6355441349761418\t|\n",
            "|\t14\t|\t0.6332182195628928\t|\n",
            "|\t15\t|\t0.6310529105503055\t|\n",
            "|\t16\t|\t0.6290324723865871\t|\n",
            "|\t17\t|\t0.6271430104637657\t|\n",
            "|\t18\t|\t0.6253722341360458\t|\n",
            "|\t19\t|\t0.6237092513688858\t|\n",
            "|\t20\t|\t0.6221443909904263\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 4\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.687597277926688\t|\n",
            "|\t2\t|\t0.6825223462990505\t|\n",
            "|\t3\t|\t0.6778713493957664\t|\n",
            "|\t4\t|\t0.6735989777228345\t|\n",
            "|\t5\t|\t0.6696650877234894\t|\n",
            "|\t6\t|\t0.6660341628074347\t|\n",
            "|\t7\t|\t0.6626748071212406\t|\n",
            "|\t8\t|\t0.659559278262735\t|\n",
            "|\t9\t|\t0.656663061701758\t|\n",
            "|\t10\t|\t0.653964487235567\t|\n",
            "|\t11\t|\t0.6514443861689393\t|\n",
            "|\t12\t|\t0.6490857868773466\t|\n",
            "|\t13\t|\t0.6468736458267377\t|\n",
            "|\t14\t|\t0.6447946108567658\t|\n",
            "|\t15\t|\t0.6428368134857964\t|\n",
            "|\t16\t|\t0.6409896870904415\t|\n",
            "|\t17\t|\t0.639243807994629\t|\n",
            "|\t18\t|\t0.6375907567340543\t|\n",
            "|\t19\t|\t0.6360229970141623\t|\n",
            "|\t20\t|\t0.634533770135457\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 5\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6898167105795917\t|\n",
            "|\t2\t|\t0.6867111121770711\t|\n",
            "|\t3\t|\t0.6838110449174175\t|\n",
            "|\t4\t|\t0.6810989151786823\t|\n",
            "|\t5\t|\t0.6785587414762478\t|\n",
            "|\t6\t|\t0.6761760224157547\t|\n",
            "|\t7\t|\t0.6739376094988366\t|\n",
            "|\t8\t|\t0.6718315863340065\t|\n",
            "|\t9\t|\t0.6698471552485767\t|\n",
            "|\t10\t|\t0.6679745318511815\t|\n",
            "|\t11\t|\t0.6662048477479396\t|\n",
            "|\t12\t|\t0.6645300613554064\t|\n",
            "|\t13\t|\t0.6629428765663418\t|\n",
            "|\t14\t|\t0.6614366688963733\t|\n",
            "|\t15\t|\t0.6600054186585451\t|\n",
            "|\t16\t|\t0.6586436506677302\t|\n",
            "|\t17\t|\t0.6573463799588685\t|\n",
            "|\t18\t|\t0.656109063004552\t|\n",
            "|\t19\t|\t0.6549275539327177\t|\n",
            "|\t20\t|\t0.6537980652694867\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 6\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6841054444388254\t|\n",
            "|\t2\t|\t0.675917726662234\t|\n",
            "|\t3\t|\t0.6684891430003687\t|\n",
            "|\t4\t|\t0.661733981912885\t|\n",
            "|\t5\t|\t0.6555755974742744\t|\n",
            "|\t6\t|\t0.6499459791914789\t|\n",
            "|\t7\t|\t0.6447851209846069\t|\n",
            "|\t8\t|\t0.6400402873779517\t|\n",
            "|\t9\t|\t0.6356652484848093\t|\n",
            "|\t10\t|\t0.6316195317127611\t|\n",
            "|\t11\t|\t0.627867719308876\t|\n",
            "|\t12\t|\t0.6243788071115729\t|\n",
            "|\t13\t|\t0.6211256304920423\t|\n",
            "|\t14\t|\t0.6180843574818927\t|\n",
            "|\t15\t|\t0.6152340455555351\t|\n",
            "|\t16\t|\t0.6125562566878809\t|\n",
            "|\t17\t|\t0.6100347245511977\t|\n",
            "|\t18\t|\t0.6076550676294342\t|\n",
            "|\t19\t|\t0.6054045423282893\t|\n",
            "|\t20\t|\t0.6032718306579181\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 7\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6830503344113232\t|\n",
            "|\t2\t|\t0.674115001393894\t|\n",
            "|\t3\t|\t0.6661796285262366\t|\n",
            "|\t4\t|\t0.6591036835622278\t|\n",
            "|\t5\t|\t0.6527661956758725\t|\n",
            "|\t6\t|\t0.6470638202536497\t|\n",
            "|\t7\t|\t0.641908718711178\t|\n",
            "|\t8\t|\t0.6372264559661468\t|\n",
            "|\t9\t|\t0.6329540385786975\t|\n",
            "|\t10\t|\t0.6290381557968835\t|\n",
            "|\t11\t|\t0.6254336446077059\t|\n",
            "|\t12\t|\t0.6221021748682806\t|\n",
            "|\t13\t|\t0.6190111371290759\t|\n",
            "|\t14\t|\t0.6161327098468562\t|\n",
            "|\t15\t|\t0.6134430813088068\t|\n",
            "|\t16\t|\t0.6109218027056165\t|\n",
            "|\t17\t|\t0.6085512511060365\t|\n",
            "|\t18\t|\t0.6063161838301612\t|\n",
            "|\t19\t|\t0.6042033684675632\t|\n",
            "|\t20\t|\t0.6022012753251879\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 8\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6892580184915378\t|\n",
            "|\t2\t|\t0.6856439704307828\t|\n",
            "|\t3\t|\t0.6822800696894339\t|\n",
            "|\t4\t|\t0.6791438602042252\t|\n",
            "|\t5\t|\t0.6762151463899093\t|\n",
            "|\t6\t|\t0.67347576304934\t|\n",
            "|\t7\t|\t0.670909365427749\t|\n",
            "|\t8\t|\t0.668501238850412\t|\n",
            "|\t9\t|\t0.666238126968615\t|\n",
            "|\t10\t|\t0.664108077394866\t|\n",
            "|\t11\t|\t0.6621003033857992\t|\n",
            "|\t12\t|\t0.6602050601931895\t|\n",
            "|\t13\t|\t0.6584135347224989\t|\n",
            "|\t14\t|\t0.6567177471942869\t|\n",
            "|\t15\t|\t0.6551104635821383\t|\n",
            "|\t16\t|\t0.6535851176912117\t|\n",
            "|\t17\t|\t0.6521357418369508\t|\n",
            "|\t18\t|\t0.6507569051790547\t|\n",
            "|\t19\t|\t0.6494436588583375\t|\n",
            "|\t20\t|\t0.6481914871715976\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 9\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6877793382809394\t|\n",
            "|\t2\t|\t0.6830385747414623\t|\n",
            "|\t3\t|\t0.6788362908770748\t|\n",
            "|\t4\t|\t0.6750969130071717\t|\n",
            "|\t5\t|\t0.6717561019688718\t|\n",
            "|\t6\t|\t0.6687591346729765\t|\n",
            "|\t7\t|\t0.6660594744528929\t|\n",
            "|\t8\t|\t0.6636175284115361\t|\n",
            "|\t9\t|\t0.6613995796856709\t|\n",
            "|\t10\t|\t0.6593768775021157\t|\n",
            "|\t11\t|\t0.6575248661570463\t|\n",
            "|\t12\t|\t0.6558225342631008\t|\n",
            "|\t13\t|\t0.654251866887636\t|\n",
            "|\t14\t|\t0.6527973849832328\t|\n",
            "|\t15\t|\t0.6514457584449679\t|\n",
            "|\t16\t|\t0.650185481021418\t|\n",
            "|\t17\t|\t0.6490065970547534\t|\n",
            "|\t18\t|\t0.6479004715839938\t|\n",
            "|\t19\t|\t0.6468595967028754\t|\n",
            "|\t20\t|\t0.6458774282269255\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "Execution Time:  62.288098096847534 \n",
            "\n",
            "+-------+------------+-----------+--------+-----------+\n",
            "| Class |  Accuracy  | Precision | Recall |  F1-Score |\n",
            "+-------+------------+-----------+--------+-----------+\n",
            "|   0   |  0.7483    |  0.2790   | 0.9847 |  0.4348   |\n",
            "|   1   |  0.6854    |  0.2622   | 0.9973 |  0.4152   |\n",
            "|   2   |  0.7116    |  0.2582   | 0.9765 |  0.4084   |\n",
            "|   3   |  0.7114    |  0.2596   | 0.9728 |  0.4099   |\n",
            "|   4   |  0.6783    |  0.2210   | 0.9956 |  0.3617   |\n",
            "|   5   |  0.6614    |  0.2088   | 0.9737 |  0.3438   |\n",
            "|   6   |  0.7522    |  0.2862   | 0.9935 |  0.4444   |\n",
            "|   7   |  0.7211    |  0.2772   | 0.9875 |  0.4329   |\n",
            "|   8   |  0.6673    |  0.2180   | 0.9587 |  0.3552   |\n",
            "|   9   |  0.6915    |  0.2406   | 0.9833 |  0.3866   |\n",
            "+-------+------------+-----------+--------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iv. Logistic Regression using Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "339M4ch9V0LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "stopping_criterion = 1e-6\n",
        "batch_size = 1000\n",
        "max_iters = 20\n",
        "\n",
        "start_time = time.time()\n",
        "accuracies, precisions, recalls, f1_scores = logistic_regression_one_vs_rest(X_train_normalized, y_train, X_test_normalized, y_test, learning_rate, stopping_criterion, batch_size, max_iters)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Display results\n",
        "print(\"\\nExecution Time: \", execution_time, \"\\n\")\n",
        "print(\"+-------+------------+-----------+--------+-----------+\")\n",
        "print(\"| Class |  Accuracy  | Precision | Recall |  F1-Score |\")\n",
        "print(\"+-------+------------+-----------+--------+-----------+\")\n",
        "for i in range(10):\n",
        "    print(f\"|   {i}   |  {accuracies[i]:.4f}    |  {precisions[i]:.4f}   | {recalls[i]:.4f} |  {f1_scores[i]:.4f}   |\")\n",
        "print(\"+-------+------------+-----------+--------+-----------+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "280fxRs9V6De",
        "outputId": "4a8e1595-e197-4d4a-de11-fdb40f240531"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t\tTraining for class 0\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6777574451143046\t|\n",
            "|\t2\t|\t0.6631691853061964\t|\n",
            "|\t3\t|\t0.6504047092357471\t|\n",
            "|\t4\t|\t0.6411404234361422\t|\n",
            "|\t5\t|\t0.6327874479916622\t|\n",
            "|\t6\t|\t0.626314230205495\t|\n",
            "|\t7\t|\t0.6208479139342598\t|\n",
            "|\t8\t|\t0.6156811573205712\t|\n",
            "|\t9\t|\t0.6104755324909369\t|\n",
            "|\t10\t|\t0.6063022830941204\t|\n",
            "|\t11\t|\t0.6023525451539274\t|\n",
            "|\t12\t|\t0.5987786259366692\t|\n",
            "|\t13\t|\t0.5955858850621808\t|\n",
            "|\t14\t|\t0.592603988216997\t|\n",
            "|\t15\t|\t0.5895727337708618\t|\n",
            "|\t16\t|\t0.5870638024119704\t|\n",
            "|\t17\t|\t0.5843161452459757\t|\n",
            "|\t18\t|\t0.5819338245503629\t|\n",
            "|\t19\t|\t0.5798823441695566\t|\n",
            "|\t20\t|\t0.5779653291389499\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 1\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6800299257736239\t|\n",
            "|\t2\t|\t0.66930088699236\t|\n",
            "|\t3\t|\t0.6597198231750777\t|\n",
            "|\t4\t|\t0.6523286850259823\t|\n",
            "|\t5\t|\t0.6448999426492573\t|\n",
            "|\t6\t|\t0.638064791852722\t|\n",
            "|\t7\t|\t0.63246870636708\t|\n",
            "|\t8\t|\t0.6277847335917026\t|\n",
            "|\t9\t|\t0.6225382424531674\t|\n",
            "|\t10\t|\t0.618969436076308\t|\n",
            "|\t11\t|\t0.615298209031896\t|\n",
            "|\t12\t|\t0.6123338729548907\t|\n",
            "|\t13\t|\t0.6092901925794788\t|\n",
            "|\t14\t|\t0.6062031156958008\t|\n",
            "|\t15\t|\t0.6038168378606036\t|\n",
            "|\t16\t|\t0.6013827475333985\t|\n",
            "|\t17\t|\t0.5988709189902058\t|\n",
            "|\t18\t|\t0.5969784430614123\t|\n",
            "|\t19\t|\t0.5948458210462932\t|\n",
            "|\t20\t|\t0.5930261362155238\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 2\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6853699366197337\t|\n",
            "|\t2\t|\t0.6776177984616547\t|\n",
            "|\t3\t|\t0.6711185642847649\t|\n",
            "|\t4\t|\t0.665429792991289\t|\n",
            "|\t5\t|\t0.660184755105428\t|\n",
            "|\t6\t|\t0.6553350107388516\t|\n",
            "|\t7\t|\t0.6503970061739762\t|\n",
            "|\t8\t|\t0.6463917759816744\t|\n",
            "|\t9\t|\t0.6429140332693827\t|\n",
            "|\t10\t|\t0.6394952359459846\t|\n",
            "|\t11\t|\t0.6365263190243005\t|\n",
            "|\t12\t|\t0.6335315350779115\t|\n",
            "|\t13\t|\t0.6309247937506117\t|\n",
            "|\t14\t|\t0.628497425424214\t|\n",
            "|\t15\t|\t0.6261588103701321\t|\n",
            "|\t16\t|\t0.6239034121472651\t|\n",
            "|\t17\t|\t0.6214272722984786\t|\n",
            "|\t18\t|\t0.6191609216811235\t|\n",
            "|\t19\t|\t0.6172639583591363\t|\n",
            "|\t20\t|\t0.6155339468157703\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 3\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6851448239877974\t|\n",
            "|\t2\t|\t0.6792579675528465\t|\n",
            "|\t3\t|\t0.6729653007385448\t|\n",
            "|\t4\t|\t0.667221648710349\t|\n",
            "|\t5\t|\t0.6622650125937225\t|\n",
            "|\t6\t|\t0.6573247456367881\t|\n",
            "|\t7\t|\t0.6526881190314187\t|\n",
            "|\t8\t|\t0.6489156058563501\t|\n",
            "|\t9\t|\t0.6458612206645792\t|\n",
            "|\t10\t|\t0.6428473385552291\t|\n",
            "|\t11\t|\t0.640037997949939\t|\n",
            "|\t12\t|\t0.6371459016272798\t|\n",
            "|\t13\t|\t0.6355520300090289\t|\n",
            "|\t14\t|\t0.6334093266623626\t|\n",
            "|\t15\t|\t0.6313149005333326\t|\n",
            "|\t16\t|\t0.629148759110196\t|\n",
            "|\t17\t|\t0.6267566164381633\t|\n",
            "|\t18\t|\t0.6249745329960886\t|\n",
            "|\t19\t|\t0.6234609197665705\t|\n",
            "|\t20\t|\t0.621961353542505\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 4\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6875459866939491\t|\n",
            "|\t2\t|\t0.6826304154934031\t|\n",
            "|\t3\t|\t0.6778798849101012\t|\n",
            "|\t4\t|\t0.6733743233421213\t|\n",
            "|\t5\t|\t0.66976152858652\t|\n",
            "|\t6\t|\t0.6664185190709808\t|\n",
            "|\t7\t|\t0.6625529048449185\t|\n",
            "|\t8\t|\t0.6594831617753053\t|\n",
            "|\t9\t|\t0.6563663990956161\t|\n",
            "|\t10\t|\t0.6536093601254025\t|\n",
            "|\t11\t|\t0.6510940482746194\t|\n",
            "|\t12\t|\t0.6488679771901563\t|\n",
            "|\t13\t|\t0.6466561017598892\t|\n",
            "|\t14\t|\t0.6443537129354246\t|\n",
            "|\t15\t|\t0.6424346462016515\t|\n",
            "|\t16\t|\t0.6404858747851134\t|\n",
            "|\t17\t|\t0.6386254942154436\t|\n",
            "|\t18\t|\t0.6370305638762628\t|\n",
            "|\t19\t|\t0.6358200174661752\t|\n",
            "|\t20\t|\t0.6344263905084102\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 5\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6898331381456762\t|\n",
            "|\t2\t|\t0.6868253998038631\t|\n",
            "|\t3\t|\t0.6842608898419148\t|\n",
            "|\t4\t|\t0.6815797879726452\t|\n",
            "|\t5\t|\t0.6789361985975813\t|\n",
            "|\t6\t|\t0.6764836888911153\t|\n",
            "|\t7\t|\t0.6742841873361798\t|\n",
            "|\t8\t|\t0.6721690560399779\t|\n",
            "|\t9\t|\t0.6701901297717426\t|\n",
            "|\t10\t|\t0.6681007790697279\t|\n",
            "|\t11\t|\t0.6662981792645339\t|\n",
            "|\t12\t|\t0.6642122028264694\t|\n",
            "|\t13\t|\t0.6624887322145627\t|\n",
            "|\t14\t|\t0.6609779567576455\t|\n",
            "|\t15\t|\t0.6596205806804722\t|\n",
            "|\t16\t|\t0.6583948863112714\t|\n",
            "|\t17\t|\t0.6571429988923339\t|\n",
            "|\t18\t|\t0.6558632953133986\t|\n",
            "|\t19\t|\t0.65486724886159\t|\n",
            "|\t20\t|\t0.6537508214662847\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 6\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.684194874655906\t|\n",
            "|\t2\t|\t0.6765274324663177\t|\n",
            "|\t3\t|\t0.6684533549125199\t|\n",
            "|\t4\t|\t0.661990237835042\t|\n",
            "|\t5\t|\t0.6557555747868735\t|\n",
            "|\t6\t|\t0.6501536719843088\t|\n",
            "|\t7\t|\t0.6452393047080124\t|\n",
            "|\t8\t|\t0.6401138921666466\t|\n",
            "|\t9\t|\t0.6359068629845426\t|\n",
            "|\t10\t|\t0.6319946870112408\t|\n",
            "|\t11\t|\t0.628526371244466\t|\n",
            "|\t12\t|\t0.6249383893158417\t|\n",
            "|\t13\t|\t0.6216234597662835\t|\n",
            "|\t14\t|\t0.6184446702756948\t|\n",
            "|\t15\t|\t0.6154204746181701\t|\n",
            "|\t16\t|\t0.6126400914588548\t|\n",
            "|\t17\t|\t0.6103533103302565\t|\n",
            "|\t18\t|\t0.6081055384460535\t|\n",
            "|\t19\t|\t0.6058971516739807\t|\n",
            "|\t20\t|\t0.6038695715186744\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 7\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6832933411011145\t|\n",
            "|\t2\t|\t0.6734654620576088\t|\n",
            "|\t3\t|\t0.6662325475180583\t|\n",
            "|\t4\t|\t0.6592380557583626\t|\n",
            "|\t5\t|\t0.6524994242865287\t|\n",
            "|\t6\t|\t0.6465384794075372\t|\n",
            "|\t7\t|\t0.6415337519462161\t|\n",
            "|\t8\t|\t0.6373936485652029\t|\n",
            "|\t9\t|\t0.6328857255191996\t|\n",
            "|\t10\t|\t0.6292893870351208\t|\n",
            "|\t11\t|\t0.6254246664852383\t|\n",
            "|\t12\t|\t0.6221339162190745\t|\n",
            "|\t13\t|\t0.6188977971339749\t|\n",
            "|\t14\t|\t0.6161788028163289\t|\n",
            "|\t15\t|\t0.6134659265728776\t|\n",
            "|\t16\t|\t0.6110699133984655\t|\n",
            "|\t17\t|\t0.608609306724787\t|\n",
            "|\t18\t|\t0.606500566317668\t|\n",
            "|\t19\t|\t0.6042331684876268\t|\n",
            "|\t20\t|\t0.6023136586201601\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 8\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6893700383862816\t|\n",
            "|\t2\t|\t0.685370644271512\t|\n",
            "|\t3\t|\t0.6828465470324145\t|\n",
            "|\t4\t|\t0.6796546998277823\t|\n",
            "|\t5\t|\t0.6765788454537891\t|\n",
            "|\t6\t|\t0.6738334232146169\t|\n",
            "|\t7\t|\t0.6714725598829485\t|\n",
            "|\t8\t|\t0.6689702329754591\t|\n",
            "|\t9\t|\t0.6670628364010041\t|\n",
            "|\t10\t|\t0.664943916164547\t|\n",
            "|\t11\t|\t0.6625030375243359\t|\n",
            "|\t12\t|\t0.660308027713106\t|\n",
            "|\t13\t|\t0.6582654462458598\t|\n",
            "|\t14\t|\t0.6562626312471062\t|\n",
            "|\t15\t|\t0.6545681897496912\t|\n",
            "|\t16\t|\t0.6528374310483959\t|\n",
            "|\t17\t|\t0.6513102718861353\t|\n",
            "|\t18\t|\t0.6500134851150722\t|\n",
            "|\t19\t|\t0.6489674385659683\t|\n",
            "|\t20\t|\t0.6480293870645918\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "\t\tTraining for class 9\n",
            "\n",
            "+---------------+-------------------------------+\n",
            "|\tEpoch\t|\t\tLoss\t\t|\n",
            "+---------------+-------------------------------+\n",
            "|\t1\t|\t0.6885057618831041\t|\n",
            "|\t2\t|\t0.6835504614388204\t|\n",
            "|\t3\t|\t0.679257965586622\t|\n",
            "|\t4\t|\t0.6761511902243378\t|\n",
            "|\t5\t|\t0.6727721554684799\t|\n",
            "|\t6\t|\t0.6694870385261429\t|\n",
            "|\t7\t|\t0.6670127281585121\t|\n",
            "|\t8\t|\t0.6641315967947584\t|\n",
            "|\t9\t|\t0.6616917682469402\t|\n",
            "|\t10\t|\t0.6594387547448399\t|\n",
            "|\t11\t|\t0.6574251255439958\t|\n",
            "|\t12\t|\t0.6558404919797579\t|\n",
            "|\t13\t|\t0.654367095693484\t|\n",
            "|\t14\t|\t0.6526540621176499\t|\n",
            "|\t15\t|\t0.6516498879710846\t|\n",
            "|\t16\t|\t0.65062530772125\t|\n",
            "|\t17\t|\t0.6492287695820934\t|\n",
            "|\t18\t|\t0.6481445487632853\t|\n",
            "|\t19\t|\t0.6470486472630326\t|\n",
            "|\t20\t|\t0.6458818612879419\t|\n",
            "+---------------+-------------------------------+\n",
            "\n",
            "Execution Time:  11.355380535125732 \n",
            "\n",
            "+-------+------------+-----------+--------+-----------+\n",
            "| Class |  Accuracy  | Precision | Recall |  F1-Score |\n",
            "+-------+------------+-----------+--------+-----------+\n",
            "|   0   |  0.7452    |  0.2764   | 0.9837 |  0.4316   |\n",
            "|   1   |  0.6793    |  0.2584   | 0.9969 |  0.4105   |\n",
            "|   2   |  0.7088    |  0.2563   | 0.9760 |  0.4060   |\n",
            "|   3   |  0.7120    |  0.2602   | 0.9738 |  0.4107   |\n",
            "|   4   |  0.6778    |  0.2207   | 0.9951 |  0.3613   |\n",
            "|   5   |  0.6597    |  0.2079   | 0.9737 |  0.3427   |\n",
            "|   6   |  0.7490    |  0.2836   | 0.9940 |  0.4413   |\n",
            "|   7   |  0.7147    |  0.2728   | 0.9884 |  0.4275   |\n",
            "|   8   |  0.6647    |  0.2168   | 0.9597 |  0.3537   |\n",
            "|   9   |  0.6952    |  0.2427   | 0.9818 |  0.3892   |\n",
            "+-------+------------+-----------+--------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Results\n",
        "\n",
        "Using a **learning rate of 0.01** over **20 Epochs** for both vanilla and stochastic gradient descent, the comparison is as follows:\n",
        "\n",
        "| Gradient Descent Type | Execution Time (s) | Class | Accuracy | Precision | Recall | F1-Score |\n",
        "|---|---|---|---|---|---|---|\n",
        "| Vanilla | 62.28 |   0   |  0.7483    |  0.2790   | 0.9847 |  0.4348   |\n",
        "|  |  |   1   |  0.6854    |  0.2622   | 0.9973 |  0.4152   |\n",
        "|  |  |   2   |  0.7116    |  0.2582   | 0.9765 |  0.4084   |\n",
        "|  |  |   3   |  0.7114    |  0.2596   | 0.9728 |  0.4099   |\n",
        "|  |  |   4   |  0.6783    |  0.2210   | 0.9956 |  0.3617   |\n",
        "|  |  |   5   |  0.6614    |  0.2088   | 0.9737 |  0.3438   |\n",
        "|  |  |   6   |  0.7522    |  0.2862   | 0.9935 |  0.4444   |\n",
        "|  |  |   7   |  0.7211    |  0.2772   | 0.9875 |  0.4329   |\n",
        "|  |  |   8   |  0.6673    |  0.2180   | 0.9587 |  0.3552   |\n",
        "|  |  |   9   |  0.6915    |  0.2406   | 0.9833 |  0.3866   |\n",
        "| Vanilla | 11.35 |   0   |  0.7452    |  0.2764   | 0.9837 |  0.4316   |\n",
        "|  |  |   1   |  0.6793    |  0.2584   | 0.9969 |  0.4105   |\n",
        "|  |  |   2   |  0.7088    |  0.2563   | 0.9760 |  0.4060   |\n",
        "|  |  |   3   |  0.7120    |  0.2602   | 0.9738 |  0.4107   |\n",
        "|  |  |   4   |  0.6778    |  0.2207   | 0.9951 |  0.3613   |\n",
        "|  |  |   5   |  0.6597    |  0.2079   | 0.9737 |  0.3427   |\n",
        "|  |  |   6   |  0.7490    |  0.2836   | 0.9940 |  0.4413   |\n",
        "|  |  |   7   |  0.7147    |  0.2728   | 0.9884 |  0.4275   |\n",
        "|  |  |   8   |  0.6647    |  0.2168   | 0.9597 |  0.3537   |\n",
        "|  |  |   9   |  0.6952    |  0.2427   | 0.9818 |  0.3892   |\n",
        "\n",
        "**Execution Time**: SGD is significantly faster than vanilla GD. The execution time for SGD is much lower compared to vanilla GD, indicating that SGD converges faster due to its update frequency with smaller batches.\n",
        "\n",
        "**Accuracy**: The accuracy values for both vanilla GD and SGD are similar across different classes, with SGD sometimes performing slightly better. This suggests that both methods are effective for this classification task.\n",
        "\n",
        "**Precision, Recall, and F1-Score**: The precision, recall, and F1-score values for both vanilla GD and SGD are consistent across different classes. There is no significant difference in performance between the two methods in terms of these metrics.\n",
        "\n",
        "**Overall Performance**: Both vanilla GD and SGD perform well on the MNIST dataset, achieving high accuracy, precision, recall, and F1-scores across different classes. However, SGD offers the advantage of faster convergence, making it a more efficient choice for large datasets.\n",
        "\n",
        "##Conclusion\n",
        "\n",
        "In summary, based on this data, we can conclude that SGD is a preferable choice over vanilla GD for logistic regression on the MNIST dataset due to its faster convergence and similar performance in terms of accuracy and other metrics."
      ],
      "metadata": {
        "id": "cNbSP7SUYOSz"
      }
    }
  ]
}